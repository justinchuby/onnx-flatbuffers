# automatically generated by the FlatBuffers compiler, do not modify

# namespace: onnx

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class ShardingSpec(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = ShardingSpec()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsShardingSpec(cls, buf, offset=0):
        """This method is deprecated. Please switch to GetRootAs."""
        return cls.GetRootAs(buf, offset)
    # ShardingSpec
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # ShardingSpec
    def TensorName(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.String(o + self._tab.Pos)
        return None

    # ShardingSpec
    def Device(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            a = self._tab.Vector(o)
            return self._tab.Get(flatbuffers.number_types.Int64Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 8))
        return 0

    # ShardingSpec
    def DeviceAsNumpy(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Int64Flags, o)
        return 0

    # ShardingSpec
    def DeviceLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def DeviceIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        return o == 0

    # ShardingSpec
    def IndexToDeviceGroupMap(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            x = self._tab.Vector(o)
            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
            x = self._tab.Indirect(x)
            from onnx.IntIntListEntry import IntIntListEntry
            obj = IntIntListEntry()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # ShardingSpec
    def IndexToDeviceGroupMapLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def IndexToDeviceGroupMapIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        return o == 0

    # ShardingSpec
    def ShardedDim(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            x = self._tab.Vector(o)
            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
            x = self._tab.Indirect(x)
            from onnx.ShardedDim import ShardedDim
            obj = ShardedDim()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # ShardingSpec
    def ShardedDimLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def ShardedDimIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        return o == 0

def ShardingSpecStart(builder):
    builder.StartObject(4)

def Start(builder):
    ShardingSpecStart(builder)

def ShardingSpecAddTensorName(builder, tensorName):
    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(tensorName), 0)

def AddTensorName(builder, tensorName):
    ShardingSpecAddTensorName(builder, tensorName)

def ShardingSpecAddDevice(builder, device):
    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(device), 0)

def AddDevice(builder, device):
    ShardingSpecAddDevice(builder, device)

def ShardingSpecStartDeviceVector(builder, numElems):
    return builder.StartVector(8, numElems, 8)

def StartDeviceVector(builder, numElems):
    return ShardingSpecStartDeviceVector(builder, numElems)

def ShardingSpecAddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap):
    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(indexToDeviceGroupMap), 0)

def AddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap):
    ShardingSpecAddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap)

def ShardingSpecStartIndexToDeviceGroupMapVector(builder, numElems):
    return builder.StartVector(4, numElems, 4)

def StartIndexToDeviceGroupMapVector(builder, numElems):
    return ShardingSpecStartIndexToDeviceGroupMapVector(builder, numElems)

def ShardingSpecAddShardedDim(builder, shardedDim):
    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(shardedDim), 0)

def AddShardedDim(builder, shardedDim):
    ShardingSpecAddShardedDim(builder, shardedDim)

def ShardingSpecStartShardedDimVector(builder, numElems):
    return builder.StartVector(4, numElems, 4)

def StartShardedDimVector(builder, numElems):
    return ShardingSpecStartShardedDimVector(builder, numElems)

def ShardingSpecEnd(builder):
    return builder.EndObject()

def End(builder):
    return ShardingSpecEnd(builder)
