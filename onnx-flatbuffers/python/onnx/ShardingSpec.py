# automatically generated by the FlatBuffers compiler, do not modify

# namespace: onnx

import flatbuffers
from flatbuffers.compat import import_numpy
np = import_numpy()

class ShardingSpec(object):
    __slots__ = ['_tab']

    @classmethod
    def GetRootAs(cls, buf, offset=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, offset)
        x = ShardingSpec()
        x.Init(buf, n + offset)
        return x

    @classmethod
    def GetRootAsShardingSpec(cls, buf, offset=0):
        """This method is deprecated. Please switch to GetRootAs."""
        return cls.GetRootAs(buf, offset)
    # ShardingSpec
    def Init(self, buf, pos):
        self._tab = flatbuffers.table.Table(buf, pos)

    # ShardingSpec
    def TensorName(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(4))
        if o != 0:
            return self._tab.String(o + self._tab.Pos)
        return None

    # ShardingSpec
    def Device(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            a = self._tab.Vector(o)
            return self._tab.Get(flatbuffers.number_types.Int64Flags, a + flatbuffers.number_types.UOffsetTFlags.py_type(j * 8))
        return 0

    # ShardingSpec
    def DeviceAsNumpy(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.GetVectorAsNumpy(flatbuffers.number_types.Int64Flags, o)
        return 0

    # ShardingSpec
    def DeviceLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def DeviceIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(6))
        return o == 0

    # ShardingSpec
    def IndexToDeviceGroupMap(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            x = self._tab.Vector(o)
            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
            x = self._tab.Indirect(x)
            from onnx.IntIntListEntry import IntIntListEntry
            obj = IntIntListEntry()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # ShardingSpec
    def IndexToDeviceGroupMapLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def IndexToDeviceGroupMapIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(8))
        return o == 0

    # ShardingSpec
    def ShardedDim(self, j):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            x = self._tab.Vector(o)
            x += flatbuffers.number_types.UOffsetTFlags.py_type(j) * 4
            x = self._tab.Indirect(x)
            from onnx.ShardedDim import ShardedDim
            obj = ShardedDim()
            obj.Init(self._tab.Bytes, x)
            return obj
        return None

    # ShardingSpec
    def ShardedDimLength(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        if o != 0:
            return self._tab.VectorLen(o)
        return 0

    # ShardingSpec
    def ShardedDimIsNone(self):
        o = flatbuffers.number_types.UOffsetTFlags.py_type(self._tab.Offset(10))
        return o == 0

def ShardingSpecStart(builder):
    builder.StartObject(4)

def Start(builder):
    ShardingSpecStart(builder)

def ShardingSpecAddTensorName(builder, tensorName):
    builder.PrependUOffsetTRelativeSlot(0, flatbuffers.number_types.UOffsetTFlags.py_type(tensorName), 0)

def AddTensorName(builder, tensorName):
    ShardingSpecAddTensorName(builder, tensorName)

def ShardingSpecAddDevice(builder, device):
    builder.PrependUOffsetTRelativeSlot(1, flatbuffers.number_types.UOffsetTFlags.py_type(device), 0)

def AddDevice(builder, device):
    ShardingSpecAddDevice(builder, device)

def ShardingSpecStartDeviceVector(builder, numElems):
    return builder.StartVector(8, numElems, 8)

def StartDeviceVector(builder, numElems):
    return ShardingSpecStartDeviceVector(builder, numElems)

def ShardingSpecAddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap):
    builder.PrependUOffsetTRelativeSlot(2, flatbuffers.number_types.UOffsetTFlags.py_type(indexToDeviceGroupMap), 0)

def AddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap):
    ShardingSpecAddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap)

def ShardingSpecStartIndexToDeviceGroupMapVector(builder, numElems):
    return builder.StartVector(4, numElems, 4)

def StartIndexToDeviceGroupMapVector(builder, numElems):
    return ShardingSpecStartIndexToDeviceGroupMapVector(builder, numElems)

def ShardingSpecAddShardedDim(builder, shardedDim):
    builder.PrependUOffsetTRelativeSlot(3, flatbuffers.number_types.UOffsetTFlags.py_type(shardedDim), 0)

def AddShardedDim(builder, shardedDim):
    ShardingSpecAddShardedDim(builder, shardedDim)

def ShardingSpecStartShardedDimVector(builder, numElems):
    return builder.StartVector(4, numElems, 4)

def StartShardedDimVector(builder, numElems):
    return ShardingSpecStartShardedDimVector(builder, numElems)

def ShardingSpecEnd(builder):
    return builder.EndObject()

def End(builder):
    return ShardingSpecEnd(builder)

import onnx.IntIntListEntry
import onnx.ShardedDim
try:
    from typing import List
except:
    pass

class ShardingSpecT(object):

    # ShardingSpecT
    def __init__(self):
        self.tensorName = None  # type: str
        self.device = None  # type: List[int]
        self.indexToDeviceGroupMap = None  # type: List[onnx.IntIntListEntry.IntIntListEntryT]
        self.shardedDim = None  # type: List[onnx.ShardedDim.ShardedDimT]

    @classmethod
    def InitFromBuf(cls, buf, pos):
        shardingSpec = ShardingSpec()
        shardingSpec.Init(buf, pos)
        return cls.InitFromObj(shardingSpec)

    @classmethod
    def InitFromPackedBuf(cls, buf, pos=0):
        n = flatbuffers.encode.Get(flatbuffers.packer.uoffset, buf, pos)
        return cls.InitFromBuf(buf, pos+n)

    @classmethod
    def InitFromObj(cls, shardingSpec):
        x = ShardingSpecT()
        x._UnPack(shardingSpec)
        return x

    # ShardingSpecT
    def _UnPack(self, shardingSpec):
        if shardingSpec is None:
            return
        self.tensorName = shardingSpec.TensorName()
        if not shardingSpec.DeviceIsNone():
            if np is None:
                self.device = []
                for i in range(shardingSpec.DeviceLength()):
                    self.device.append(shardingSpec.Device(i))
            else:
                self.device = shardingSpec.DeviceAsNumpy()
        if not shardingSpec.IndexToDeviceGroupMapIsNone():
            self.indexToDeviceGroupMap = []
            for i in range(shardingSpec.IndexToDeviceGroupMapLength()):
                if shardingSpec.IndexToDeviceGroupMap(i) is None:
                    self.indexToDeviceGroupMap.append(None)
                else:
                    intIntListEntry_ = onnx.IntIntListEntry.IntIntListEntryT.InitFromObj(shardingSpec.IndexToDeviceGroupMap(i))
                    self.indexToDeviceGroupMap.append(intIntListEntry_)
        if not shardingSpec.ShardedDimIsNone():
            self.shardedDim = []
            for i in range(shardingSpec.ShardedDimLength()):
                if shardingSpec.ShardedDim(i) is None:
                    self.shardedDim.append(None)
                else:
                    shardedDim_ = onnx.ShardedDim.ShardedDimT.InitFromObj(shardingSpec.ShardedDim(i))
                    self.shardedDim.append(shardedDim_)

    # ShardingSpecT
    def Pack(self, builder):
        if self.tensorName is not None:
            tensorName = builder.CreateString(self.tensorName)
        if self.device is not None:
            if np is not None and type(self.device) is np.ndarray:
                device = builder.CreateNumpyVector(self.device)
            else:
                ShardingSpecStartDeviceVector(builder, len(self.device))
                for i in reversed(range(len(self.device))):
                    builder.PrependInt64(self.device[i])
                device = builder.EndVector()
        if self.indexToDeviceGroupMap is not None:
            indexToDeviceGroupMaplist = []
            for i in range(len(self.indexToDeviceGroupMap)):
                indexToDeviceGroupMaplist.append(self.indexToDeviceGroupMap[i].Pack(builder))
            ShardingSpecStartIndexToDeviceGroupMapVector(builder, len(self.indexToDeviceGroupMap))
            for i in reversed(range(len(self.indexToDeviceGroupMap))):
                builder.PrependUOffsetTRelative(indexToDeviceGroupMaplist[i])
            indexToDeviceGroupMap = builder.EndVector()
        if self.shardedDim is not None:
            shardedDimlist = []
            for i in range(len(self.shardedDim)):
                shardedDimlist.append(self.shardedDim[i].Pack(builder))
            ShardingSpecStartShardedDimVector(builder, len(self.shardedDim))
            for i in reversed(range(len(self.shardedDim))):
                builder.PrependUOffsetTRelative(shardedDimlist[i])
            shardedDim = builder.EndVector()
        ShardingSpecStart(builder)
        if self.tensorName is not None:
            ShardingSpecAddTensorName(builder, tensorName)
        if self.device is not None:
            ShardingSpecAddDevice(builder, device)
        if self.indexToDeviceGroupMap is not None:
            ShardingSpecAddIndexToDeviceGroupMap(builder, indexToDeviceGroupMap)
        if self.shardedDim is not None:
            ShardingSpecAddShardedDim(builder, shardedDim)
        shardingSpec = ShardingSpecEnd(builder)
        return shardingSpec
